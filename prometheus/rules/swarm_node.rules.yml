groups:
- name: swarm_node_alerts
  rules:
  # CPU Alerts
  - alert: NodeHighCPU
    expr: 100 - (avg(irate(node_cpu_seconds_total{mode="idle"}[1m]) * ON(instance) GROUP_LEFT(node_name) node_meta * 100) BY (node_name)) > 85
    for: 5m
    labels:
      severity: warning
    annotations:
      description: Swarm node {{ $labels.node_name }} CPU usage is at {{ humanize $value}}%.
      summary: High CPU usage on node '{{ $labels.node_name }}'

  - alert: NodeCriticalCPU
    expr: 100 - (avg(irate(node_cpu_seconds_total{mode="idle"}[1m]) * ON(instance) GROUP_LEFT(node_name) node_meta * 100) BY (node_name)) > 95
    for: 2m
    labels:
      severity: critical
    annotations:
      description: Swarm node {{ $labels.node_name }} CPU usage is critically high at {{ humanize $value}}%.
      summary: Critical CPU usage on node '{{ $labels.node_name }}'

  # Memory Alerts
  - alert: NodeHighMemory
    expr: sum(((node_memory_MemTotal_bytes - node_memory_MemAvailable_bytes) / node_memory_MemTotal_bytes) * ON(instance) GROUP_LEFT(node_name) node_meta * 100) BY (node_name) > 85
    for: 5m
    labels:
      severity: warning
    annotations:
      description: Swarm node {{ $labels.node_name }} memory usage is at {{ humanize $value}}%.
      summary: High memory usage on node '{{ $labels.node_name }}'

  - alert: NodeCriticalMemory
    expr: sum(((node_memory_MemTotal_bytes - node_memory_MemAvailable_bytes) / node_memory_MemTotal_bytes) * ON(instance) GROUP_LEFT(node_name) node_meta * 100) BY (node_name) > 95
    for: 2m
    labels:
      severity: critical
    annotations:
      description: Swarm node {{ $labels.node_name }} memory usage is critically high at {{ humanize $value}}%.
      summary: Critical memory usage on node '{{ $labels.node_name }}'

  # Disk Alerts
  - alert: NodeHighDiskUsage
    expr: ((node_filesystem_size_bytes{mountpoint="/"} - node_filesystem_free_bytes{mountpoint="/"}) * 100 / node_filesystem_size_bytes{mountpoint="/"}) * ON(instance) GROUP_LEFT(node_name) node_meta > 85
    for: 5m
    labels:
      severity: warning
    annotations:
      description: Swarm node {{ $labels.node_name }} disk usage is at {{ humanize $value}}%.
      summary: High disk usage on node '{{ $labels.node_name }}'

  - alert: NodeCriticalDiskUsage
    expr: ((node_filesystem_size_bytes{mountpoint="/"} - node_filesystem_free_bytes{mountpoint="/"}) * 100 / node_filesystem_size_bytes{mountpoint="/"}) * ON(instance) GROUP_LEFT(node_name) node_meta > 95
    for: 2m
    labels:
      severity: critical
    annotations:
      description: Swarm node {{ $labels.node_name }} disk usage is critically high at {{ humanize $value}}%.
      summary: Critical disk usage on node '{{ $labels.node_name }}'

  - alert: NodeDiskFillRate6h
    expr: predict_linear(node_filesystem_free_bytes{mountpoint="/"}[1h], 6 * 3600) * ON(instance) GROUP_LEFT(node_name) node_meta < 0
    for: 1h
    labels:
      severity: critical
    annotations:
      description: Swarm node {{ $labels.node_name }} disk will fill up within 6 hours.
      summary: Disk fill rate alert for node '{{ $labels.node_name }}'

  # Load Average Alert
  - alert: NodeHighLoad
    expr: (node_load5 * ON(instance) GROUP_LEFT(node_name) node_meta) / ON(instance) GROUP_LEFT() count(node_cpu_seconds_total{mode="idle"}) BY (instance) > 2
    for: 10m
    labels:
      severity: warning
    annotations:
      description: Swarm node {{ $labels.node_name }} load average (5m) is {{ humanize $value}} times the CPU count.
      summary: High load average on node '{{ $labels.node_name }}'

  # Swap Usage Alert
  - alert: NodeSwapUsage
    expr: ((node_memory_SwapTotal_bytes - node_memory_SwapFree_bytes) / node_memory_SwapTotal_bytes * 100) * ON(instance) GROUP_LEFT(node_name) node_meta > 50
    for: 5m
    labels:
      severity: warning
    annotations:
      description: Swarm node {{ $labels.node_name }} swap usage is at {{ humanize $value}}%.
      summary: Swap usage alert on node '{{ $labels.node_name }}'

  # File Descriptor Alert
  - alert: NodeFileDescriptorLimit
    expr: (node_filefd_allocated / node_filefd_maximum * 100) * ON(instance) GROUP_LEFT(node_name) node_meta > 80
    for: 5m
    labels:
      severity: warning
    annotations:
      description: Swarm node {{ $labels.node_name }} file descriptor usage is at {{ humanize $value}}%.
      summary: File descriptor limit warning on node '{{ $labels.node_name }}'

  # Network Errors Alert
  - alert: NodeNetworkErrors
    expr: increase(node_network_receive_errs_total[5m]) * ON(instance) GROUP_LEFT(node_name) node_meta > 10
    for: 5m
    labels:
      severity: warning
    annotations:
      description: Swarm node {{ $labels.node_name }} has network receive errors.
      summary: Network errors on node '{{ $labels.node_name }}'

  # Disk I/O Alert
  - alert: NodeDiskIOHigh
    expr: (avg(irate(node_cpu_seconds_total{mode="iowait"}[5m])) BY (instance) * 100) * ON(instance) GROUP_LEFT(node_name) node_meta > 30
    for: 10m
    labels:
      severity: warning
    annotations:
      description: Swarm node {{ $labels.node_name }} I/O wait is at {{ humanize $value}}%.
      summary: High disk I/O on node '{{ $labels.node_name }}'

  # Node Reboot Detection
  - alert: NodeReboot
    expr: (time() - node_boot_time_seconds) * ON(instance) GROUP_LEFT(node_name) node_meta < 300
    for: 1m
    labels:
      severity: info
    annotations:
      description: Swarm node {{ $labels.node_name }} was recently rebooted.
      summary: Node reboot detected on '{{ $labels.node_name }}'

  # Cluster Health
  - alert: NodesMissingFromCluster
    expr: sum(node_uname_info) < 11
    for: 10m
    labels:
      severity: critical
    annotations:
      description: Expected 11 nodes but only {{ $value }} are reporting. One or more nodes are missing from the swarm.
      summary: Nodes missing from swarm cluster

  # Scrape Target Down
  - alert: ScrapeTargetDown
    expr: up == 0
    for: 2m
    labels:
      severity: critical
    annotations:
      description: Prometheus scrape target {{ $labels.job }}/{{ $labels.instance }} is down.
      summary: Scrape target down - {{ $labels.job }}
